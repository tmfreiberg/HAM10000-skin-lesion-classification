{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9f32ebf",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92e86b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path['project'] : D:\\projects\\skin-lesion-classification\n",
      "path['images'] : D:\\projects\\skin-lesion-classification\\images\n",
      "path['models'] : D:\\projects\\skin-lesion-classification\\models\n",
      "path['expository'] : D:\\projects\\skin-lesion-classification\\expository\n",
      "path['literature'] : D:\\projects\\skin-lesion-classification\\literature\n",
      "path['notebooks'] : D:\\projects\\skin-lesion-classification\\notebooks\n",
      "path['presentation'] : D:\\projects\\skin-lesion-classification\\presentation\n",
      "path['scripts'] : D:\\projects\\skin-lesion-classification\\scripts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# If we're using Google Colab, we set the environment variable to point to the relevant folder in our Google Drive:\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    os.environ['SKIN_LESION_CLASSIFICATION'] = '/content/drive/MyDrive/Colab Notebooks/skin-lesion-classification'\n",
    "\n",
    "# Otherwise, we use the environment variable on our local system:\n",
    "project_environment_variable = \"SKIN_LESION_CLASSIFICATION\"\n",
    "\n",
    "# Path to the root directory of the project:\n",
    "project_path = Path(os.environ.get(project_environment_variable))\n",
    "\n",
    "# Relative path to /scripts (from where custom modules will be imported):\n",
    "scripts_path = project_path.joinpath(\"scripts\")\n",
    "\n",
    "# Add this path to sys.path so that Python will look there for modules:\n",
    "sys.path.append(str(scripts_path))\n",
    "\n",
    "# Now import path_step from our custom utils module to create a dictionary to all subdirectories in our root directory:\n",
    "from utils import path_setup\n",
    "path = path_setup.subfolders(project_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af769c",
   "metadata": {},
   "source": [
    "<a id='baseline_models'></a>\n",
    "# Baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5f08207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Type, Union      # For type hints\n",
    "from processing import process      # Custom module for processing metadata\n",
    "\n",
    "data_dir: Path = path[\"images\"]     # Path to directory containing metadata.csv file\n",
    "csv_filename: str = \"metadata.csv\"  # The filename\n",
    "    \n",
    "tvr: int = 3              # Ratio of training set to validation set. See discussion below for explanation.\n",
    "seed: int = 0             # Random seed for parts of the process where randomness is called for.\n",
    "keep_first: bool = False  # If False, then, for each lesion, we choose a random image to assign to our training set. \n",
    "stratified: bool = True   # If True, we stratify classes so that the proportions remain as stable as possible after train/val split. \n",
    "                          # If False, the proportions will be roughly similar.\n",
    "\n",
    "to_classify: Union[list, dict] = [\"mel\",   # These are the lesion types we are interested in classifying. \n",
    "                                  \"bcc\",   # Any missing ones will be grouped together as the 0-label class: no need to write \"other\" here.\n",
    "                                  \"akiec\", # If 'other' is not desired, use restrict_to attribute above\n",
    "                                  \"nv\",]   # Can also be a dictionary, like { 'malignant' : ['mel', 'bcc'], 'benign' : ['nv', 'bkl']}\n",
    "    \n",
    "train_one_img_per_lesion: Union[None, bool] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2e2538b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Loaded file 'D:\\projects\\skin-lesion-classification\\images\\metadata.csv'.\n",
      "- Inserted 'num_images' column in dataframe, to the right of 'lesion_id' column.\n",
      "- Inserted 'label' column in dataframe, to the right of 'dx' column: \n",
      "  {'bkl': 0, 'df': 0, 'vasc': 0, 'akiec': 1, 'mel': 2, 'nv': 3, 'bcc': 4}\n",
      "- Added 'set' column to dataframe, with values 't1', 'v1', 'ta', and 'va', to the right of 'localization' column.\n",
      "- Basic, overall dataframe (pre-train/test split): self.df\n",
      "- Training set (not balanced, one image per lesion): self.df_train\n",
      "- Validation set (not expanded, one image per lesion): self.df_val1\n",
      "- Validation set (not expanded, use all images of each lesion): self.df_val_a\n",
      "- Small sample dataframes for code testing: self._df_train_code_test, self._df_val1_code_test, self._df_val_a_code_test\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the process class with attribute values as above.\n",
    "rn18_defaults = process(data_dir=data_dir,\n",
    "                        csv_filename=csv_filename,\n",
    "                        tvr=tvr,\n",
    "                        seed=seed,\n",
    "                        keep_first=keep_first,\n",
    "                        stratified=stratified,\n",
    "                        to_classify=to_classify,\n",
    "                        train_one_img_per_lesion=train_one_img_per_lesion,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a312c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "transforms.Resize((224,224)), # Resize images to fit ResNet input size\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize with ImageNet stats\n",
    "])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812d42e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Union, List, Callable\n",
    "import torchvision.models as models\n",
    "\n",
    "source: Union[process, pd.DataFrame] = rn18_defaults  # Processed data to be fed into model for training.\n",
    "                                                      # Must either be an instance of the process class, or a dataframe of the same format as source.df if source were an instance of the process class.\n",
    "model_dir: Path = path[\"models\"]                      # Path to directory where models/model info/model results are stored.\n",
    "    \n",
    "transform: Union[None, \n",
    "                 transforms.Compose, \n",
    "                 List[Callable]] = transform     # Transform to be applied to images before feeding into neural network.\n",
    "    \n",
    "filename_stem: Union[None, str] = \"rn18\"         # For saving model and related files. Default \"rn18\" (if ResNet model) or \"EffNet\" (if EfficientNet), or \"cnn\".\n",
    "filename_suffix: Union[None, str] = \"defaults\"   # Something descriptive and unique for future reference. Default empty string \"\".\n",
    "\n",
    "# model: Union[None, models.ResNet, models.EfficientNet] = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT) # Pre-trained model. Default: ResNet18.   \n",
    "model: Union[None, models.ResNet, models.EfficientNet] = models.resnet18(weights=\"ResNet18_Weights.DEFAULT\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f6549485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New files will be created. \n",
      "Base filename: rn18_t1_10e_defaults_00\n",
      "Attributes saved to file: D:\\projects\\skin-lesion-classification\\models\\rn18_t1_10e_defaults_00_attributes.json\n"
     ]
    }
   ],
   "source": [
    "# Create an instance of the resnet18 class with attribute values as above.\n",
    "from multiclass_models import cnn\n",
    "\n",
    "rn18_defaults = cnn(source=source,                                           \n",
    "                    model_dir=model_dir,\n",
    "                    transform=transform,\n",
    "                    filename_stem=filename_stem,\n",
    "                    filename_suffix=filename_suffix,                         \n",
    "                    model=model,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d8f19b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model.state_dict() as D:\\projects\\skin-lesion-classification\\models\\rn18_t1_10e_defaults_00.pth.\n"
     ]
    }
   ],
   "source": [
    "# from utils import print_header\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "instance = rn18_defaults\n",
    "\n",
    "model = models.resnet18()  \n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, len(instance.label_codes))\n",
    "\n",
    "# state_dict = torch.load(file_path_pth)\n",
    "# model.load_state_dict(state_dict)\n",
    "\n",
    "# model = models.efficientnet_b0()  \n",
    "# num_ftrs = model.classifier[1].in_features\n",
    "# model.classifier[1] = nn.Linear(num_ftrs, len(instance.label_codes))\n",
    "\n",
    "instance.model = model\n",
    "\n",
    "file_path = instance.model_dir.joinpath(instance._filename + \".pth\")\n",
    "print(f\"Saving model.state_dict() as {file_path}.\")\n",
    "torch.save(model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d6b6675b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================\n",
      "MODEL ARCHITECTURE\n",
      "==================\n",
      "\n",
      "NOTE: 'OUT_FEATURES = 5' AT THE END\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from utils import print_header\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "instance = rn18_defaults\n",
    "\n",
    "if instance.state_dict is None:\n",
    "    print(\"Loading model and state dictionary from file\\n\".upper())\n",
    "    file_path_pth = instance.model_dir.joinpath(instance._filename + \".pth\")\n",
    "\n",
    "    # model = models.efficientnet_b0()  \n",
    "    model = models.resnet18()  \n",
    "    if isinstance(model,models.ResNet):\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Linear(num_ftrs, len(instance.label_codes))\n",
    "    elif isinstance(model,models.EfficientNet):\n",
    "        num_ftrs = model.classifier[1].in_features\n",
    "        model.classifier[1] = nn.Linear(num_ftrs, len(instance.label_codes))\n",
    "\n",
    "    # Load the state dictionary into the model\n",
    "    state_dict = torch.load(file_path_pth)\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "    instance.model = model\n",
    "    instance.state_dict = state_dict\n",
    "    \n",
    "print_header(\"Model architecture\")\n",
    "print(f\"Note: \\'out_features = {len(instance.label_codes)}\\' at the end\".upper())\n",
    "display(instance.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4f89e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import print_header\n",
    "from multiclass_models import get_probabilities\n",
    "\n",
    "instance = rn18_defaults\n",
    "\n",
    "instance.df_probabilities_val1 = get_probabilities(df=instance.df_val1,\n",
    "                                                   data_dir=instance.data_dir,\n",
    "                                                   model_dir=instance.model_dir,\n",
    "                                                   model=instance.model,\n",
    "                                                   filename=instance._filename,\n",
    "                                                   label_codes=instance.label_codes,\n",
    "                                                   transform=instance.transform,\n",
    "                                                   batch_size=instance.batch_size,\n",
    "                                                   Print=False,\n",
    "                                                   save_as=instance._filename + \"_val1\",)\n",
    "\n",
    "instance.df_probabilities_val_a = get_probabilities(df=instance.df_val_a,\n",
    "                                                    data_dir=instance.data_dir,\n",
    "                                                    model_dir=instance.model_dir,\n",
    "                                                    model=instance.model,\n",
    "                                                    filename=instance._filename,\n",
    "                                                    label_codes=instance.label_codes,\n",
    "                                                    transform=instance.transform,\n",
    "                                                    batch_size=instance.batch_size,\n",
    "                                                    Print=False,\n",
    "                                                    save_as=instance._filename + \"_val_a\",)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bb08376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path1 = instance.model_dir.joinpath(\"rn18_t1_10e_defaults_00_val1_probabilities.csv\")\n",
    "file_path_a = instance.model_dir.joinpath(\"rn18_t1_10e_defaults_00_val_a_probabilities.csv\")\n",
    "\n",
    "instance.df_probabilities_val1 = pd.read_csv(file_path1, index_col=0)\n",
    "instance.df_probabilities_val_a = pd.read_csv(file_path_a, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a8096a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===\n",
      "...\n",
      "===\n",
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['image_id'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [96]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m print_header(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m display_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlesion_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdx\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m instance\u001b[38;5;241m.\u001b[39mdf_probabilities_val1\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprob\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m----> 3\u001b[0m display(\u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_probabilities_val1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdisplay_columns\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n\u001b[0;32m      5\u001b[0m print_header(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m display(instance\u001b[38;5;241m.\u001b[39mdf_probabilities_val_a[display_columns]\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py:3511\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3510\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 3511\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   3513\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   5779\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 5782\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5784\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   5785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   5786\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   5842\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   5844\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 5845\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['image_id'] not in index\""
     ]
    }
   ],
   "source": [
    "print_header(\"...\")\n",
    "display_columns = ['lesion_id', 'image_id', 'dx'] + [col for col in instance.df_probabilities_val1.columns if col.startswith('prob')]\n",
    "display(instance.df_probabilities_val1[display_columns].head())\n",
    "\n",
    "print_header(\"...\")\n",
    "display(instance.df_probabilities_val_a[display_columns].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c397a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cb0b791",
   "metadata": {},
   "source": [
    "<a id='models_with'></a>\n",
    "# Models with balancing\n",
    "↑↑ [Contents](#contents) ↑ [Baseline models](#baseline_models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feffd04b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98fa3a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3717b1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756af68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
